{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mV-ROgsSbED1"},"outputs":[],"source":["# using SBERT as a classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjAI2hM1bUIB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682459610337,"user_tz":300,"elapsed":32055,"user":{"displayName":"Samiha Sharmin Shimmi","userId":"10952472062329875127"}},"outputId":"b75e5811-7a20-43cb-da21-67d58ed597fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting memory_profiler\n","  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n","Collecting datasets\n","  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from memory_profiler) (5.9.5)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: tokenizers, xxhash, multidict, memory_profiler, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 memory_profiler-0.61.0 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n"]}],"source":["!pip install torch transformers memory_profiler datasets\n","import time\n","import datetime\n","tic = time.time()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ryyizELcdJe"},"outputs":[],"source":["import torch\n","import random\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from datasets import load_metric\n","%load_ext memory_profiler\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":858,"status":"ok","timestamp":1682459692327,"user":{"displayName":"Samiha Sharmin Shimmi","userId":"10952472062329875127"},"user_tz":300},"id":"IJnr-C95cirF","outputId":"1f2f3f8f-168f-40e3-cd7f-671aa2c19713"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1682459662054,"user":{"displayName":"Samiha Sharmin Shimmi","userId":"10952472062329875127"},"user_tz":300},"id":"E0wRYRu9cwBS","outputId":"580082f0-e84f-4949-cea1-b1c141e5914d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ReportCC++/SBERT\n"]}],"source":["cd drive/MyDrive/ReportCC++/SBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6T1RIOBeNUB"},"outputs":[],"source":["dfNew = pd.read_json(\"function.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zuoyrm-yqXyu"},"outputs":[],"source":["df = pd.DataFrame(dfNew, columns=['target', 'func'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1YTYl5VrJ0P"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roKne3kMc1bc"},"outputs":[],"source":["# df = pd.read_csv(\"bbc-text.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBeMt9redOfT"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDja_zW_dU34"},"outputs":[],"source":["def set_seed(seed: int):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  if is_torch_available():\n","      torch.manual_seed(seed)\n","      torch.cuda.manual_seed_all(seed)\n","\n","  if is_tf_available():\n","      import tensorflow as tf\n","\n","      tf.random.set_seed(seed)\n","\n","set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnRZN9SSdZ--"},"outputs":[],"source":["def TextClassification_with_Transformer(model_name: str, Data: pd.Series, Target:pd.Series, test_size: np.float64, max_length: int, num_labels: int, num_epochs: int, metrics_name: str):\n","\n","  # Make data\n","  X = Data\n","  y = Target\n","  y = pd.factorize(y)[0]\n","\n","  # Load Metrics\n","  metric = load_metric(metrics_name)\n","\n","  # Split Data\n","  X_train, X_test, y_train, y_test = train_test_split(X.tolist(), y, test_size=test_size)\n","\n","  # Call the Tokenizer\n","  tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n","  # Encode the text\n","  train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n","  valid_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n","\n","\n","\n","  class MakeTorchData(torch.utils.data.Dataset):\n","      def __init__(self, encodings, labels):\n","          self.encodings = encodings\n","          self.labels = labels\n","\n","      def __getitem__(self, idx):\n","          item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n","          item[\"labels\"] = torch.tensor([self.labels[idx]])\n","          return item\n","\n","      def __len__(self):\n","          return len(self.labels)\n","\n","  # convert our tokenized data into a torch Dataset\n","  train_dataset = MakeTorchData(train_encodings, y_train.ravel())\n","  valid_dataset = MakeTorchData(valid_encodings, y_test.ravel())\n","\n","\n","  # Call Model\n","  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels).to(\"cuda\")\n","\n","  # Create Metrics\n","  def compute_metrics(eval_pred):\n","\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # 'micro', 'macro', etc. are for multi-label classification. If you are running a binary classification, leave it as default or specify \"binary\" for average\n","    #return metric.compute(predictions=predictions, references=labels, average=\"micro\")\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","  # Specifiy the arguments for the trainer\n","  training_args = TrainingArguments(\n","      output_dir='./results',          # output directory\n","      num_train_epochs=num_epochs,     # total number of training epochs\n","      per_device_train_batch_size=8,   # batch size per device during training\n","      per_device_eval_batch_size=20,   # batch size for evaluation\n","      warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","      weight_decay=0.01,               # strength of weight decay\n","      logging_dir='./logs',            # directory for storing logs\n","      load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n","      metric_for_best_model = metrics_name,    # select the base metrics\n","      logging_steps=200,               # log & save weights each logging_steps\n","      save_steps=200,\n","      evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n","  )\n","\n","  # Call the Trainer\n","  trainer = Trainer(\n","      model=model,                         # the instantiated Transformers model to be trained\n","      args=training_args,                  # training arguments, defined above\n","      train_dataset=train_dataset,         # training dataset\n","      eval_dataset=valid_dataset,          # evaluation dataset\n","      compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n","  )\n","\n","  # Train the model\n","  trainer.train()\n","\n","  # Call the summary\n","  trainer.evaluate()\n","\n","\n","\n","  return trainer, model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZRWHV3ydc2k"},"outputs":[],"source":["%%time\n","%%memit\n","sbert_trainer, sbert_model = TextClassification_with_Transformer(model_name = 'sentence-transformers/all-mpnet-base-v2',\n","                                                                 Data = df.func,\n","                                                                 Target = df.target,\n","                                                                 test_size = 0.10,\n","                                                                 max_length = 512,\n","                                                                 num_labels =2,\n","                                                                 num_epochs = 3,\n","                                                                 metrics_name='precision')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}